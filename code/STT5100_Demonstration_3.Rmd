---
title: 'STT5100: Démonstration 3'
author: "Alexandre LeBlanc"
params:
  data: base_devoir_1
output:
  html_document: default
  html_notebook: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=TRUE, fig.align="center")
```

# Exercices sur la régression linéaire simple et la régression linéaire multiple

---

### Remarque

Notez que les dernières questions n'ont pas de solutions puisque vous avez abordé la séléction de modèle la semaine dernière, alors celles--ci seront abordées la semaine prochaine lors de la prochaine démonstration. *Par contre, je vous encourage de les essayer!*

---

## Base de données - Devoir #1 (H2019)

---

La base de données utilisée par la présente démonstration est disponible au répertoire Github d'Arthur Charpentier: <https://github.com/freakonometrics/STT5100/blob/master/archives/H2019/code/STT5100-H2019-devoir1.md>.

Pour télécharger la base de données effectuez le code suivant:

```{r, results="hide"}
code_permanent = "ABCD12345678"
loc_fichier = paste("http://freakonometrics.free.fr/",code_permanent,".RData",sep="")
download.file(loc_fichier, "base_devoir_1.RData")
load("base_devoir_1.RData")
dim(database)
```

```{r}
attach(database)
str(database)
```

## Question 1

Pour le modèle $E(\texttt{Prix}_{i} \mid \texttt{Surface_RdC}_{i}) = \beta_{0} + \beta_{1}\texttt{Surface_RdC}_{i},$ calculez un intervale de **prédiction** si $\texttt{Surface_RdC} = 1\,372.$  

```{r}
lin = lm(Prix ~ Surface_RdC, data = database)
predict(lin, newdata = data.frame(Surface_RdC = 1372), interval = "predict")
```

## Question 2

Supposons que l'on a posé le modèle $E(y{i} \mid \mathbf{X}) = \beta_{0} + \beta_{1}X_{i,1}$ tandis que le *vrai* modèle est $E(y_{i} \mid \mathbf{X}) = \beta_{0} + \beta_{1}X_{i,1} + \beta_{2}X_{i,2}.$ Est--ce que l'estimateur du paramètre $\widehat{\beta}_{1}$ du modèle posé est sans biais? Sinon, quel est son biais?

\begin{align*}
  E(\widehat{\beta}_{1}) &= E\left(\frac{\sum_{i = 1}^{n}(x_{i,1} - \overline{x}_{1})(y_{i} - \overline{y})}{\sum_{i = 1}^{n}(x_{i,1} - \overline{x}_{1})^2}\right)\\
  &= E\left(\frac{\sum_{i = 1}^{n}(x_{i,1} - \overline{x}_{1})y_{i}}{\sum_{i = 1}^{n}(x_{i,1} - \overline{x}_{1})^2}\right)\\
  &= E\left(\sum_{i = 1}^{n}\omega_{i}y_{i}\right) && \text{ où }\omega_{i} = \frac{x_{i,1} - \overline{x}_{1}}{\sum_{i = 1}^{n}(x_{i,1} - \overline{x}_{1})^2}\\
  &=  \sum_{i = 1}^{n}\omega_{i}E(y_{i})\\
  &= \sum_{i = 1}^{n}\omega_{i}(\beta_{0} + \beta_{1}x_{i,1} + \beta_{2}x_{i,2})\\
  &= \beta_{1} + \sum_{i = 1}^{n} \beta_{2}\omega_{i}x_{i,2}\\
  &= \beta_{1} + \frac{\sum_{i = 1}^{n}\beta_{2}(x_{i,1} - \overline{x}_{1})(x_{i,2} - \overline{x}_{2})}{\sum_{i = 1}^{n}(x_{i,1} - \overline{x}_{1})^2}\\
  &= \beta_{1} + \beta_{2}\frac{S_{x_{1}x_{2}}}{S_{x_{1}x_{1}}}
\end{align*}

Le biais de l'estimateur de $\widehat{\beta}_{1}$ du modèle réduit est $-\displaystyle\beta_{2}\frac{S_{x_{1}x_{2}}}{S_{x_{1}x_{1}}}.$

## Question 3

Calculez la variance des résidus $\widehat{{\varepsilon}} = \mathbf{y} - \widehat{\mathbf{y}}$ pour le modèle de régression linéaire. Est-elle constante?

\begin{align*}
  \text{Var}(\widehat{{\varepsilon}}) &= \text{Var}(\mathbf{y} - \widehat{\mathbf{y}})\\
                                      &= \text{Var}(\mathbf{y} - \mathbf{H}\mathbf{y})\\
                                      &= \text{Var}[(\mathbf{I}_{n} - \mathbf{H})\mathbf{y}]\\
                                      &= (\mathbf{I}_{n} - \mathbf{H})\text{Var}(\mathbf{y})(\mathbf{I}_{n} - \mathbf{H})^{T}\\
                                      &= (\mathbf{I}_{n} - \mathbf{H})\sigma^{2}\mathbf{I}_{n}(\mathbf{I}_{n} - \mathbf{H})^{T}\\
                                      &= \sigma^{2}(\mathbf{I}_{n} - \mathbf{H})^2 && \text{ puisque } \mathbf{I}_{n} - \mathbf{H} \text{ est symmétrique}\\
                                      &= \sigma^{2}(\mathbf{I}_{n} - \mathbf{H}) && \text{ puisque } \mathbf{I}_{n} - \mathbf{H} \text{ est idempotente (vérifiez!)}
\end{align*}

La variance des résidus n'est pas constante!

## Question 4

Pour le modèle de la question 1, calculez un intervalle de confiance pour l'estimé $\widehat{y}_{52}$ par le boostrap: tirez des échantillons de paires $(x_{i}, y_{i})$ avec remise. Pour chaque intervalle, tirer un échantillon de $1\,000$ et comparez les intervalles.

**Méthode de réflection:** 

Différente de la méthode présentée en classe, on calcule un intervalle à l'aide du bootsrap pour chaque paramètre et tous ces intervalles nous permettront de calculer un intervalle de $\widehat{y}.$ C'est--à--dire, on pose l'intervalle $\widehat{\beta} - D_{2} \leq \beta \leq \widehat{\beta} + D_{1}$ où $D_{1} = \widehat{\beta} - \widehat{\beta}^{*}\left(\frac{\alpha}{2}\right)$ et $D_{2} = \widehat{\beta}^{*}\left(1 - \frac{\alpha}{2}\right) - \widehat{\beta}.$ Notez que le $\widehat{\beta}^{*}$ est un quantile empirique calculé à partir de l'échantillon du bootstrap. 

Alors, pour un paramètre choisi et un alpha donné, $$2\widehat{\beta} - \widehat{\beta}^{*}\left(1 - \frac{\alpha}{2}\right) \leq \beta \leq 2\widehat{\beta} - \widehat{\beta}^{*}\left(\frac{\alpha}{2}\right)$$ 

```{r}
beta = matrix(NA, 1000, 2)
for(s in 1:1000){
  set.seed(s)
  idx = sample(1:nrow(database), nrow(database), replace = TRUE)
  reg_sim = lm(Prix ~ Surface_RdC, data=database[idx,])
  beta[s,] = reg_sim$coefficients
}

intervalle_b0 = c(2 * lin$coefficients[1] - quantile(beta[,1], 0.975), 2 * lin$coefficients[1] - quantile(beta[,1], 0.025))
intervalle_b1 = c(2 * lin$coefficients[2] - quantile(beta[,2], 0.975), 2 * lin$coefficients[2] - quantile(beta[,2], 0.025))

intervalle_y1 = intervalle_b0 + database$Surface_RdC[52] * intervalle_b1
names(intervalle_y1) <- c("2.5%", "97.5%")
intervalle_y1
```

**Méthode dans les notes de cours:**

La méthode de les notes cours calcule les quantiles non sur les betas individuels, mais plutôt sur les prévisions du modèles. Or, $$\widehat{y}^{*}\left(\frac{\alpha}{2}\right) \leq \widehat{y} \leq \widehat{y}^{*}\left(1 - \frac{\alpha}{2}\right)$$


```{r}
beta = matrix(NA, 1000, 2)
for(s in 1:1000){
  set.seed(s)
  idx = sample(1:nrow(database), nrow(database), replace = TRUE)
  reg_sim = lm(Prix ~ Surface_RdC, data=database[idx,])
  beta[s,] = reg_sim$coefficients
}

lin_pred = beta %*% c(1, database$Surface_RdC[52])

intervalle_y2 = c(quantile(lin_pred, 0.025), quantile(lin_pred, 0.975))
intervalle_y2
```


## Question 5

Pour le modèle de régression linéaire multiple, démontrez que le $SCR = \mathbf{y}^{T}(\textbf{I}_{n} - \mathbf{H})\mathbf{y}$ et calculer $E(SCR)$ en montrant que $E(\mathbf{y}^{T}\mathbf{A}\mathbf{y}) = \text{tr}(\mathbf{AV}) + \boldsymbol{\mu}^{T}\mathbf{A}\boldsymbol{\mu}$ pour $E(\mathbf{y}) =  \boldsymbol{\mu}$, $\text{Var}(\mathbf{y}) = \mathbf{V}$ et $\mathbf{A}$ une matrice symmétrique.

\begin{align*}
  E[(\mathbf{y} - \boldsymbol{\mu})^{T}\mathbf{A}(\mathbf{y} - \boldsymbol{\mu})] &= E\left[\sum_{i = 1}^{n}\sum_{j = 1}^{n}(y_{i} - \mu_{i})A_{i,j}(y_{j} - \mu_{j})\right]\\
  &= \sum_{i = 1}^{n}\sum_{j = 1}^{n}A_{i,j}E\left[(y_{i} - \mu_{i})(y_{j} - \mu_{j})\right]\\
  &= \sum_{i = 1}^{n}\sum_{j = 1}^{n}A_{i,j}V_{i,j}\\
  &= \sum_{i = 1}^{n}A_{i,i}V_{i,i} && \text{puisque } \mathbf{A} \text{ et } \mathbf{V} \text{ sont symmétriques}\\
  &= \text{tr}(\mathbf{AV})
\end{align*}

En développant le côté gauche de l'égalité, on obtient $E(\mathbf{y}^{T}\mathbf{A}\mathbf{y}) = \text{tr}(\mathbf{AV}) + \boldsymbol{\mu}^{T}\mathbf{A}\boldsymbol{\mu}.$ Remarquez que 

$$SCR = \displaystyle\sum_{i = 1}^{n}\widehat{\varepsilon}_{i}^2 = \widehat{\boldsymbol{\varepsilon}}^{T}\widehat{\boldsymbol{\varepsilon}} = [(\mathbf{I}_{n} - \mathbf{H})\mathbf{y}]^{T}(\mathbf{I}_{n} - \mathbf{H})\mathbf{y} = \mathbf{y}^{T}(\mathbf{I}_{n} - \mathbf{H})^{T}(\mathbf{I}_{n} - \mathbf{H})\mathbf{y} = \mathbf{y}^{T}(\mathbf{I}_{n} - \mathbf{H})\mathbf{y}$$

Finalement, 

\begin{align*}
  E(SCR) = E(\mathbf{y}^{T}(\mathbf{I}_{n} - \mathbf{H})\mathbf{y}) &= \text{tr}[(\mathbf{I}_{n} - \mathbf{H})\sigma^2\mathbf{I}_{n}] + (\mathbf{X}\boldsymbol{\beta})^{T}(\mathbf{I}_{n} - \mathbf{H})\mathbf{X}\boldsymbol{\beta}\\
  &= [\text{tr}(\mathbf{I}_{n}) - \text{tr}(\mathbf{H})]\sigma^2 + \boldsymbol{\beta}^{T}(\mathbf{X}^{T}\mathbf{X} - \mathbf{X}^{T}\mathbf{H}\mathbf{X})\boldsymbol{\beta}\\
  &= [n - \text{tr}(\mathbf{X}(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T})]\sigma^2 + \boldsymbol{\beta}^{T}(\mathbf{X}^{T}\mathbf{X} - \mathbf{X}^{T}\mathbf{X})\boldsymbol{\beta} && \text{ puisque } \mathbf{HX} = \mathbf{X} \text{ et } \mathbf{X}^{T}\mathbf{H} = \mathbf{X}^{T}\\
  &=[n - \text{tr}(\mathbf{X}\mathbf{X}^{T}(\mathbf{X}^{T}\mathbf{X})^{-1})]\sigma^2\\
  &= [n - \text{tr}(\mathbf{I}_{p})]\sigma^2\\
  &= (n - p)\sigma^2
\end{align*}

## Question 6

Supposons qu'il y a deux modèles qui respectent les hypothèses de la régression linéaire classique: 

<center>
$\text{Modèle A:} \quad \mathbf{y} = \mathbf{X}_{1}\boldsymbol{\beta}_{1} + \boldsymbol{\varepsilon} \quad \text{ et } \quad \text{Modèle B:} \quad \mathbf{y} = \mathbf{X}_{1}\boldsymbol{\beta}_{1} + \mathbf{X}_{2}\boldsymbol{\beta}_{2} + \boldsymbol{\varepsilon}$
</center>

Montrez que $R^2_{A} \leq R^2_{B}.$

Soient $\boldsymbol{\beta}_{1}$ un vecteur de $i$ paramètres et $\boldsymbol{\beta}_{2}$ est un vecteur de $j$ paramètres. Puisque $R^2 = \displaystyle\frac{RSS}{TSS},$ 

\[\underbrace{RSS(\beta_{i,1}, \beta_{i,2},\,\ldots\,, \beta_{i,i}, \beta_{j,1}, \beta_{j,2},\,\ldots\,, \beta_{j,j} \mid  \beta_{0})}_{\text{SSR du modèle B}} = \underbrace{RSS(\beta_{j,1}, \beta_{j,2},\,\ldots\,, \beta_{j,j} \mid  \beta_{i,1}, \beta_{i,2},\,\ldots\,, \beta_{i,i}, \beta_{0})}_{\text{Augmentation du SSR en ajoutant le } \boldsymbol{\beta}_{2} \text { au modèle A}} + \underbrace{RSS(\beta_{i,1}, \beta_{i,2},\,\ldots\,, \beta_{i,i} \mid \beta_{0})}_{\text{SSR du modèle A}}\]

Puisque $RSS > 0$, il s'ensuit que $R^2_{A} \leq R^2_{B}.$

## Question 7

Pour le modèle régression linéaire multiple, démontrer que $R^2 = \widehat{\text{Corr}}(\mathbf{y}, \widehat{\mathbf{y}})^2.$

\begin{align*}
  \widehat{\text{Corr}}(\mathbf{y}, \widehat{\mathbf{y}})^2 &= \frac{\widehat{\text{Cov}}(\mathbf{y}, \widehat{\mathbf{y}})^2}{\widehat{\text{Var}}(\mathbf{y})\widehat{\text{Var}}(\widehat{\mathbf{y}})}\\
  &= \frac{\widehat{\text{Cov}}(\widehat{\mathbf{y}} + \widehat{\boldsymbol{\varepsilon}}, \widehat{\mathbf{y}})^2}{\widehat{\text{Var}}(\mathbf{y})\widehat{\text{Var}}(\widehat{\mathbf{y}})}\\
  &= \frac{\left[\widehat{\text{Var}}(\widehat{\mathbf{y}}) + \widehat{\text{Cov}}(\widehat{\mathbf{y}}, \widehat{\boldsymbol{\varepsilon}})\right]^2}{\widehat{\text{Var}}(\mathbf{y})\widehat{\text{Var}}(\widehat{\mathbf{y}})}\\
  &= \frac{\widehat{\text{Var}}(\widehat{\mathbf{y}})^2}{\widehat{\text{Var}}(\mathbf{y})\widehat{\text{Var}}(\widehat{\mathbf{y}})} && \widehat{\text{Cov}}(\widehat{\mathbf{y}}, \widehat{\boldsymbol{\varepsilon}}) = 0 \text{ puisque } \sum_{i = 1}^{n}\widehat{y}_{i}\widehat{\varepsilon}_{i} = 0 \text{ et } \sum_{i = 1}^{n}\widehat{\varepsilon}_{i} = 0\\
  &= \frac{\widehat{\text{Var}}(\widehat{\mathbf{y}})}{\widehat{\text{Var}}(\mathbf{y})}\\
  &= \frac{\sum_{i = 1}^{n}(\widehat{y}_{i} - \overline{y})^2}{\sum_{i = 1}^{n}(y_{i} - \overline{y})^2}\\
  &= \frac{RSS}{TSS} = R^2
\end{align*}

## Question 8

Faire un *forward selection* et *backward selection* pour le modèle de régression linéaire multiple. Utiliser la statistique du test de Fisher avec $p_{OUT} = 0.1$ et $p_{IN} = 0.01.$ Ces dernières sont les valeurs--p selon lequelles on rejette ou on inclut une variable explicative au modèle. Comme modèle de départ pour la *forward selection*, poser la variable \texttt{Prix} contre la variable la plus corrélée avec cette dernière et pour *backward selection*, posez le modèle complet (sans interactions). Quelle est la différence du $R^2$ des deux modèles sélectionnés?

```{r}
```

## Question 9

Faire un  *stepwise selection* pour le modèle de régression linéaire multiple. Utiliser le critère AIC pour parvenir au modèle final. Utilisez encore la variable \texttt{Prix} contre la variable explicative la plus corrélée avec cette dernière comme modèle initial.

```{r}
library(MASS)
```


## Question 10

Pour le modèle de la Q9, quels sont les points qui possèdent une grande pondération (en fonction de $h_{i}$) **ainsi que** les points qui sont influent (en fonction de la distance de Cook)? Typiquement, on dit qu'une observation possède une grande pondération si $h_{i,i} > \displaystyle\frac{2p}{n}$ et influente si $D_{i} = \displaystyle\frac{\widehat{\varepsilon}_{i}^2}{p\sigma}\left[\frac{h_{i,i}}{(1 - h_{i,i})^2}\right] > 1$ (pour grand $n$).

```{r}
```
