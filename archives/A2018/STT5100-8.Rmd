---
title: "STT5100 #8 - Regression Logistique"
author: "Arthur Charpentier"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# cas covariables continues

```{r}
myocarde = read.table("http://freakonometrics.free.fr/myocarde.csv",head=TRUE, sep=";")
str(myocarde)
for(i in 1:7) boxplot(myocarde[,i]~myocarde$PRONO,horizontal=TRUE)
summary(glm(PRONO~.,data=myocarde,family=binomial(link = "logit")))
```

Estimation des paramètres par maximum de vraimsemblance : ne pas utiliser une fonction d'optimisation (directe)

```{r}
y = (myocarde$PRONO=="DECES")
X = cbind(1,as.matrix(myocarde[,1:7]))
negLogLik = function(beta){
 -sum(-y*log(1 + exp(-(X%*%beta))) - (1-y)*log(1 + exp(X%*%beta)))
}
beta_init = lm(PRONO=="DECES"~.,data=myocarde)$coefficients
logistic_opt = optim(par = beta_init, negLogLik, hessian=TRUE, method = "BFGS", control=list(abstol=1e-9))
```

On peut toutefois utiliser l'algorithme de Fisher

```{r}
Y=(myocarde$PRONO=="DECES")
X=cbind(1,as.matrix(myocarde[,1:7]))
colnames(X)=c("Inter",names(myocarde[,1:7]))
 beta=as.matrix(lm(Y~0+X)$coefficients,ncol=1)
 for(s in 1:9){
   pi=exp(X%*%beta[,s])/(1+exp(X%*%beta[,s]))
   gradient=t(X)%*%(Y-pi)
   omega=matrix(0,nrow(X),nrow(X));diag(omega)=(pi*(1-pi))
   Hessian=-t(X)%*%omega%*%X
   beta=cbind(beta,beta[,s]-solve(Hessian)%*%gradient)}
beta[,8:10]
```

(et l'estimation de l'écart type de nos estimateurs)

```{r}
Hessian
```

ou la version vue en cours (moindres carrés pondérés itérés)

```{r}
df = myocarde
beta_init = lm(PRONO=="DECES"~.,data=df)$coefficients
X = cbind(1,as.matrix(myocarde[,1:7]))
beta = beta_init
for(s in 1:1000){
p = exp(X %*% beta) / (1+exp(X %*% beta))
omega = diag(nrow(df))
diag(omega) = (p*(1-p))
df$Z = X %*% beta + solve(omega) %*% ((df$PRONO=="DECES") - p)
beta = lm(Z~.,data=df[,-8], weights=diag(omega))$coefficients
}
beta
```

On peut le faire sur un petit exemple, pour illustrer

```{r}
x1 = c(.4,.55,.65,.9,.1,.35,.5,.15,.2,.85)
x2 = c(.85,.95,.8,.87,.5,.55,.5,.2,.1,.3)
y = c(1,1,1,1,1,0,0,1,0,0)
df = data.frame(x1=x1,x2=x2,y=as.factor(y))
plot(x1,x2,pch=c(1,19)[1+y])
```

On obtient comme classification

```{r}
clr10 = rev(heat.colors(10))
reg = glm(y~x1+x2,data=df,family=binomial(link = "logit"))
u = seq(0,1,length=101)
p = function(x,y) predict.glm(reg,newdata=data.frame(x1=x,x2=y),type="response")
v = outer(u,u,p)
image(u,u,v,xlab="Variable 1",ylab="Variable 2",col=clr10,breaks=(0:10)/10)
points(x1,x2,pch=19,cex=1.5,col="white")
points(x1,x2,pch=c(1,19)[1+y],cex=1.5)
contour(u,u,v,levels = .5,add=TRUE)
```

Petite remarque

```{r}
summary(glm((PRONO=="DECES")~.,data=myocarde,family=binomial(link = "logit")))
summary(glm((PRONO=="SURVIE")~.,data=myocarde,family=binomial(link = "logit")))
```

# cas covariables catégorielles 

On obtient comme classification

```{r}
admin = read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
str(admin)
for(i in 2:4) boxplot(admin[,i]~admin$admit,horizontal=TRUE)
```
  
On peut faire un tableau de contingence

```{r}
xtabs(~admit + rank, data = admin)
```

## Commençons par le cas à deux modalités

```{r}
library(dplyr)
admin$rank = as.factor(admin$rank)
admin$rank = recode_factor(admin$rank,`1`="1+2",`2`="1+2",`3`="3+4",`4`="3+4")
str(admin)
xtabs(~admit + rank, data = admin)
chisq.test(xtabs(~admit + rank, data = admin))
t.test(admin$admit[admin$rank=="1+2"],admin$admit[admin$rank=="3+4"])
u=seq(0,.6,length=601)
p12=mean(admin$admit[admin$rank=="1+2"])
n12=sum(admin$rank=="1+2")
p34=mean(admin$admit[admin$rank=="3+4"])
n34=sum(admin$rank=="3+4")
plot(u,dnorm(u,p12,sqrt(p12*(1-p12)/n12)),col="blue",lwd=2,type="l",ylim=c(0,15))
lines(u,dnorm(u,p34,sqrt(p34*(1-p34)/n34)),col="red",lwd=2,type="l")
```

On rejette ici l'hypothèse nulle d'indépendance entre les deux variables.

On peut aussi tenter une régression logistique

```{r}
reg = glm(admit ~ rank, family = "binomial", data = admin)
summary(reg)
## CIs using profiled log-likelihood
confint(reg)
## CIs using standard errors
confint.default(reg)
library(aod)
wald.test(b = coef(reg), Sigma = vcov(reg), Terms = 2)
## odds ratios only
exp(coef(reg))
exp(cbind(OR = coef(reg), confint(reg)))
```

## avec plusieurs modalités

```{r}
admin = read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
xtabs(~admit + rank, data = admin)
chisq.test(xtabs(~admit + rank, data = admin))
reg = glm(admit ~ as.factor(rank), family = "binomial", data = admin)
summary(reg)
## CIs using profiled log-likelihood
confint(reg)
## CIs using standard errors
confint.default(reg)
library(aod)
wald.test(b = coef(reg), Sigma = vcov(reg), Terms = 2)
## odds ratios only
exp(coef(reg))
exp(cbind(OR = coef(reg), confint(reg)))
```

# Courbe ROC

```{r}
reg=glm((PRONO=="DECES")~.,data=myocarde,family=binomial(link = "logit"))
seuil = .5
score = data.frame(yobs=(myocarde$PRONO=="DECES"),ypred=predict(reg,type="response"))
head(score)
xtabs(~yobs + (ypred>seuil), data = score)
(FP=sum((score$ypred>seuil)*(score$yobs==0))/sum(score$yobs==0))
(TP=sum((score$ypred>seuil)*(score$yobs==1))/sum(score$yobs==1))
```


```{r}
roc_curve = function(seuil){
FP=sum((score$ypred>seuil)*(score$yobs==0))/sum(score$yobs==0)
TP=sum((score$ypred>seuil)*(score$yobs==1))/sum(score$yobs==1)
return(c(FPR=FP,TPR=TP))}
u=seq(0,1,by=.01)
ROC.curve=t(Vectorize(roc_curve)(u))
plot(ROC.curve,type="s")
abline(a=0,b=1,lty=2,col="red")
points(FP,TP,pch=19,col="blue")
```

```{r}
library(verification)
roc.plot(score$yobs,score$ypred)
library(ROCR)
pred = prediction(score$ypred,score$yobs)
rocs = performance(pred, "tpr", "fpr")
plot(rocs)
perf = performance(pred, measure = "auc")
perf@y.values
perf = performance(pred, "rch")
plot(perf,col="red",add=TRUE)
```

# Kolmogorov Smirnov

```{r}
f0=c(0,sort(score$ypred[score$yobs==0]),1)
f1=c(0,sort(score$ypred[score$yobs==1]),1)
plot(f0,(0:(length(f0)-1))/(length(f0)-1),col="red",type="s",xlim=0:1)
lines(f1,(0:(length(f1)-1))/(length(f1)-1),col="blue",type="s")
```

# Cas binomial

```{r}
conting = xtabs(~admit + rank, data = admin)
cont_admin = data.frame(y0=conting[1,],y1=conting[2,],x=1:4,n=apply(conting,2,sum))
cont_admin
attach(cont_admin)
reg=glm(cbind(y1, y0) ~ as.factor(x), data = cont_admin,family="binomial")
summary(reg)
```

# Cas multinomial

```{r}
library(nnet)
model=multinom(as.factor(rank) ~ gre + gpa, data = admin)
summary(model)
(z = summary(model)$coefficients/summary(model)$standard.errors)
(pval = (1 - pnorm(abs(z), 0, 1)) * 2)
exp(coef(model))
head(predict(model, data = admin, type="probs"))
```