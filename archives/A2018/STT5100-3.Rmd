---
title: "STT5100 #3 - Regression Lineaire"
author: "Arthur Charpentier"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
data(cars)
```

## Hypotheses du Modele Lineaire

$$
(y_i=\boldsymbol{x}_i^{\text{T}}\boldsymbol{\beta}+\varepsilon_i=\beta_0 + [\beta_1 x_{1,i}+\cdots+
\beta_k x_{k,i}]+\varepsilon_i)
$$
Cf <a href=https://en.wikipedia.org/wiki/Linear_regression>linear model</a></span>, avec $\varepsilon_i\sim\mathcal{N}(0,\sigma^2)$ i.id.

$$
(Y\vert \boldsymbol{X}=\boldsymbol{x})\sim
\mathcal{N}(\boldsymbol{x}^{\text{T}}\boldsymbol{\beta},
\sigma^2)
$$
i.e.
$$
\mathbb{E}[Y\vert\boldsymbol{X}=\boldsymbol{x}]=\boldsymbol{x}^{\text{T}}\boldsymbol{\beta}
$$
and homoscedastic model,
$$
\text{Var}[Y\vert\boldsymbol{X}=\boldsymbol{x}]=\sigma^2
$$

Least squares (and maximum likelihood) estimator
$$
\widehat{\boldsymbol{\beta}}=\text{argmin}
\left\lbrace
\sum_{i=1}^n (y_i-\boldsymbol{x}_i^{\text{T}}\boldsymbol{\beta})^2
\right\rbrace
=(\boldsymbol{X}^{\text{T}}\boldsymbol{X})^{-1}\boldsymbol{X}^{\text{T}}\boldsymbol{y}$$



```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(cars)
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
model <- lm(dist ~ speed, data=cars)
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
summary(model)
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
X=cbind(1,cars$speed)
Y=cars$dist
solve(crossprod(X,X),crossprod(X,Y))
model$coefficients
```



```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
summary(model)$sigma^2*solve(crossprod(X,X))
vcov(model)
n=nrow(cars)
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
confint(model)
model$coefficients[1]+qt(c(.025,.975),n-2)* summary(model)$coefficients[1,2]
```

<a href=https://en.wikipedia.org/wiki/Coefficient_of_determination>coefficient of determination</a></span> 
$\displaystyle{R^2 = \frac{\text{explained variance}}{\text{total variance}}}$

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
summary(model)$r.squared
1-deviance(model)/sum((Y-mean(Y))^2)
```

$$
\text{Var}[Y]=
\text{Var}[\mathbb{E}[Y\vert X]]+
\mathbb{E}[\text{Var}[Y\vert X]]
$$

see   <a href=https://en.wikipedia.org/wiki/Analysis_of_variance>analysis of variance</a></span>



$$
\overline{r}^2 = 
1-[1-R^2]\cdot \frac{n-1}{n-(k-1)-1}
$$

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
summary(model)$adj.r.squared
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
anova(lm(dist~speed,data=cars),lm(dist~1,data=cars))
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(cars)
abline(model,col="red")
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(cars)
abline(model,col="red")
x=seq(2,26)
y=predict(model, newdata=data.frame(speed=x))
lines(x,y,lwd=2,col="blue")
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
y=predict(model, newdata=data.frame(speed=x), interval = "confidence")
head(y)
```

$$
\text{Var}[\widehat{Y}(\boldsymbol{x})]=\text{Var}[\boldsymbol{x}^{\text{T}}\widehat{\boldsymbol{\beta}}]=\boldsymbol{x}^{\text{T}}\text{Var}[\widehat{\boldsymbol{\beta}}]\boldsymbol{x}
=\widehat{\sigma}^2 \boldsymbol{x}^{\text{T}}[\boldsymbol{X}^{\text{T}}\boldsymbol{X}]^{-1}\boldsymbol{x}
$$



```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(cars)
polygon(c(x,rev(x)),c(y[,2],rev(y[,3])),col=rgb(0,0,1,.4),border=NA)
lines(x,y[,1],lwd=2,col="blue")
```

## Incertitude dans le modele lineaire

Method 1

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
Y=matrix(NA,1000,length(x))
for(b in 1:1000){
  idx <- sample(1:nrow(cars),size=nrow(cars),replace=TRUE)
  modelb <- lm(dist ~ speed, data=cars[idx,])
  Y[b,] <- predict(modelb, newdata=data.frame(speed=x))
}
```

See <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrap</a></span>, based on pseudo-sample
$$
\lbrace(\boldsymbol{x}_{i_1},y_{i_1}),
\cdots,(\boldsymbol{x}_{i_n},y_{i_n})
\rbrace
$$
where $(i_1,\cdots,i_n)\in\lbrace 1,2,\cdots,n\rbrace$.


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(cars)
for(b in 1:100) lines(x,Y[b,],col=rgb(0,0,1,.4))
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(cars)
lines(x,apply(Y,2,mean),col="blue",lwd=2)
lines(x,apply(Y,2,function(x) quantile(x,.025)),col="red",lty=2)
lines(x,apply(Y,2,function(x) quantile(x,.975)),col="red",lty=2)
```

Method 2

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
pred_dist=predict(model)
epsilon  =residuals(model)
Y=matrix(NA,1000,length(x))
for(b in 1:1000){
  idx <- sample(1:nrow(cars),size=nrow(cars),replace=TRUE)
  carsb=data.frame(speed=cars$speed,
                   dist=pred_dist+epsilon[idx])
  modelb <- lm(dist ~ speed, data=carsb)
  Y[b,] <- predict(modelb, newdata=data.frame(speed=x))
}
```

See <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrap</a></span>, based on pseudo-sample
$$
\lbrace(\boldsymbol{x}_{1},\widehat{y}_{1}+\widehat{\varepsilon}_{i_1}),
\cdots,(\boldsymbol{x}_{n},\widehat{y}_{n}+\widehat{\varepsilon}_{i_n})
\rbrace
$$
where $(i_1,\cdots,i_n)\in\lbrace 1,2,\cdots,n\rbrace$.


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(cars)
for(b in 1:100) lines(x,Y[b,],col=rgb(0,0,1,.4))
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(cars)
lines(x,apply(Y,2,mean),col="blue",lwd=2)
lines(x,apply(Y,2,function(x) quantile(x,.025)),col="red",lty=2)
lines(x,apply(Y,2,function(x) quantile(x,.975)),col="red",lty=2)
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(cars)
abline(model,col="red")
segments(cars$speed,cars$dist,cars$speed,predict(model),col="blue")
```
## Moindre Carres ?

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
cars2=cars; cars2[,2]=cars[,2]/10
plot(cars2,ylab="dist/10")
acp=princomp(cars2)
b=acp$loadings[2,1]/acp$loadings[1,1]
a=acp$center[2]-b*acp$center[1]
abline(a,b,col="red")
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(cars2,ylab="dist/10",xlim=c(0,30),ylim=c(-1,12))
abline(a,b,col="red")
t <- acp$loadings[,1] %*% (t(cars2)-acp$center)
X1 <- acp$center[1] +t * acp$loadings[1,1]
X2 <- acp$center[2] +t * acp$loadings[2,1]
segments(cars2$speed,cars2$dist,X1,X2,col="blue")
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
  f <- function(a) sum(abs(cars$dist-(a[1]+a[2]*cars$speed))) 
  opt <- optim( c(0,0), f )$par
  plot(cars)
  abline(model, col='red', lty=2)
  abline(opt[1], opt[2],col="blue")
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
library(quantreg,verbose=FALSE,quietly=TRUE,warn.conflicts=FALSE)
plot(cars)
abline(model, col="blue")
abline(rq(dist ~ speed,data=cars, tau=.5),col="red",lty=2)
```
## Diagnostiques

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(model,which=1)
```

Scatterplot $(\widehat{Y}_i,\widehat{\varepsilon}_i)$


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(model,which=2)
```

Scatterplot $\displaystyle{\left(\widehat{\varepsilon}_{i:n},\Phi^{-1}
\left(\frac{i}{n}\right)\right)}$


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(model,which=3)
```

Scatterplot $(\widehat{Y}_i,\sqrt{\vert\widehat{\varepsilon}_i\vert})$


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(model,which=4)
```

Cook distance, 
$\displaystyle{C_i=\frac{\widehat{\varepsilon}_i^2}{p\cdot\text{MSE}}\cdot\left(\frac{H_{i,i}}{(1-H_{i,i})^2}\right)}$ with $\boldsymbol{H}=\boldsymbol{X}(\boldsymbol{X}^{\text{T}}\boldsymbol{X})^{-1}\boldsymbol{X}^{\text{T}}=[H_{i,i}]$

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
C=cooks.distance(model)
```

$H_{i,i}$ are <a href=https://en.wikipedia.org/wiki/Leverage_(statistics)>leverages</a></span>, and define Studentized residuals as
$$
\widehat{r}_i=\frac{\widehat{\varepsilon}_i}{
\widehat{\sigma} \sqrt{1-H_{i,i}}}
$$

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
rstudent(model)
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(model,which=5)
```

Scatterplot $(H_{i,i},\widehat{r, error=TRUE}_i)$


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(model,which=6)
```

Scatterplot $(H_{i,i},C_i)$



```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
hist(residuals(model),probability=TRUE, col="light green")
lines(density(residuals(model)),lwd=2,col="red")
boxplot(residuals(model),horizontal=TRUE,add=TRUE,at=.024, 
  pars=list(boxwex=.004),col=rgb(0,0,1,.25))
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
sigma=summary(model)$sigma
plot(ecdf(residuals(model)/sigma))
lines(seq(-3,3,by=.1),pnorm(seq(-3,3,by=.1)),lwd=2,col="red")
```

<a href=https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test>Kolmogorov-Smirnov</a></span> $
d=\sup_{x\in\mathbb{r, error=TRUE}}\lbrace \vert\Phi(x)-\widehat{F}_n(x)\vert \rbrace
$

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
ks.test(residuals(model)/sigma,"pnorm")
```

<a href=https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test>Anderson-Darling</a></span>, <a href=https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion>Cramer-von Mises</a></span>, 


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
library(nortest,verbose=FALSE,quietly=TRUE,warn.conflicts=FALSE)
ad.test(residuals(model))
cvm.test(residuals(model))
```
## Choix de modele / Choix de variables

<a href=https://en.wikipedia.org/wiki/Akaike_information_criterion>AIC</a></span> 
$AIC = 2k - 2\log(\mathcal{L}) =   2k + n\left[\log\left(2\pi \frac{1}{n}\sum_{i=1}^n \widehat{\varepsilon}_i^2 \right) + 1\right]$

<a href=https://en.wikipedia.org/wiki/Bayesian_information_criterion>BIC</a></span> 
$BIC = { k \log(n) -2 \log(\mathcal{L}) }    =  k \log(n) + n\left[\log\left(2\pi \frac{1}{n}\sum_{i=1}^n \widehat{\varepsilon}_i^2 \right) + 1\right]$

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
AIC(model)
AIC(model,k=log(n))
```

## Test d'hypotheses

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
library(splines,verbose=FALSE,quietly=TRUE,warn.conflicts=FALSE)
model_brk <- lm(dist ~ bs(speed,degree=1,knots=(c(4,15,25))), data=cars)
x=seq(4,25,by=.1)
y=predict(model_brk, newdata=data.frame(speed=x))
```

see <a href=https://en.wikipedia.org/wiki/B-spline>$b$-splines</a></span>,
$$
y_i=\beta_0+\beta_1 x_i + \beta_2 (x_i-15)_+ + \varepsilon_i
$$

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(cars)
lines(x,y,lwd=2,col="blue")
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
positive=function(x,s) ifelse(x>s,x-s,0)
model_brk <- lm(dist ~ speed + positive(speed,15), data=cars)
x=seq(4,25,by=.1)
y=predict(model_brk, newdata=data.frame(speed=x))
plot(cars)
abline(model,col="red",lty=2)
lines(x,y,lwd=2,col="blue")
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(cars)
abline(model,col="red",lty=2)
lines(x,y,lwd=2,col="blue")
abline(coefficients(model_brk)[1],coefficients(model_brk)[2],col="blue",lty=2)
summary(model_brk)$coefficients
```



```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
library(strucchange,verbose=FALSE,quietly=TRUE,warn.conflicts=FALSE)
plot(Fstats(dist ~ speed,data=cars,from=7/50))
```

see <a href=https://en.wikipedia.org/wiki/Chow_test>Chow Test</a></span>


$$
W_t=\frac{1}{\widehat{\sigma}\sqrt{n}}\sum_{i=1}^{
\lfloor nt \rfloor} \widehat{\varepsilon}_i
$$

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
cusum <- efp(dist ~ speed, type = "OLS-CUSUM",data=cars)
plot(cusum,ylim=c(-2,2))
```



```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(cusum, alpha = 0.05, alt.boundary = TRUE,ylim=c(-2,2))
```

see <a href=https://en.wikipedia.org/wiki/CUSUM>CUSUM test</a></span>

## Regression avec des variables explicatives factorielles

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
model_cut=lm(dist~ cut(speed, breaks=c(0,10,15,20,25)),data=cars)
y=predict(model_cut, newdata=data.frame(speed=x))
plot(cars)
abline(model,col="red",lty=2)
lines(x,y,lwd=2,col="blue",type="s")
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
library(rpart,verbose=FALSE,quietly=TRUE,warn.conflicts=FALSE)
tree=rpart(dist~speed,data=cars)
y=predict(tree,newdata=data.frame(speed=x))
plot(cars)
abline(model,col="red",lty=2)
lines(x,y,lwd=2,col="blue",type="s")
```

## Lissage et regression polynomiale

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
model_poly=lm(dist~ poly(speed, df=3),data=cars)
y=predict(model_poly, newdata=data.frame(speed=x))
plot(cars)
abline(model,col="red",lty=2)
lines(x,y,lwd=2,col="blue")
```

## Lissage et regression locale

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
library(KernSmooth,verbose=FALSE,quietly=TRUE,warn.conflicts=FALSE)
plot(cars)
bw <- dpill(cars$speed,cars$dist) 
lines( locpoly(cars$speed,cars$dist,degree=0, bandwidth=bw), col='red' )
lines( locpoly(cars$speed,cars$dist,degree=1, bandwidth=bw), col='green' )
lines( locpoly(cars$speed,cars$dist,degree=2, bandwidth=bw), col='blue' )
```

## Lissage et $k$ plus proches voisins

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
library(FNN,verbose=FALSE,quietly=TRUE,warn.conflicts=FALSE)
knn=knn.reg(train=cars$speed,y=cars$dist,k=5)
plot(cars)
lines(cars$speed,knn$pred,col="red")
```

## Lissage et splines

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
library(splines,verbose=FALSE,quietly=TRUE,warn.conflicts=FALSE)
model_bs <- lm(dist ~ bs(speed), data=cars)
y=predict(model_bs, newdata=data.frame(speed=x))
plot(cars)
abline(model,col="red",lty=2)
lines(x,y,lwd=2,col="blue")
```

Tuning parameter selection, Silverman's Rule
$$
b^\star=0.9 \cdot\frac{ \min\lbrace \sigma,F^{-1}(.75)-F^{-1}(.25)\rbrace}{1.34 \cdot n^{1/5}}
$$

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
bw.nrd0(cars$speed)
```

See <a href=https://en.wikipedia.org/wiki/Kernel_regression>kernel regression</a></span>

Tuning parameter selection, Cross Validation
$$
b^\star=\text{argmin}\left\lbrace
\sum_{i=1}^n \left(y_i - \widehat{m}_{(i)}(\boldsymbol{x}_i)\right)^2\right\rbrace
$$

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
bw.ucv(cars$speed)
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
library(KernSmooth,verbose=FALSE,quietly=TRUE,warn.conflicts=FALSE)
Nadaraya_Watson =with(cars,ksmooth(speed, dist, "normal",bandwidth=2.75))
plot(cars)
abline(model,col="red",lty=2)
lines(Nadaraya_Watson,lwd=2,col="blue")
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
library(KernSmooth,verbose=FALSE,quietly=TRUE,warn.conflicts=FALSE)
model_loess=loess(dist~ speed, data=cars,degree=1, family="gaussian")
y=predict(model_loess, newdata=data.frame(speed=x))
plot(cars)
abline(model,col="red",lty=2)
lines(x,y,lwd=2,col="blue")
```

## Regression Multiple

Life Expectancy (1), Homicide Rate (2), Illiteracy Rate (3)

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
chicago=read.table("http://freakonometrics.free.fr/chicago.txt",header=TRUE,sep=";")
model_c = lm(Fire~X_2+X_3,data=chicago)
y=function(x2,x3) predict(model_c,newdata=data.frame(X_2=x2,X_3=x3))
VX2=seq(0,80,length=26); VX3=seq(5,25,length=26)
VY=outer(VX2,VX3,y)
persp(VX2,VX3,VY,xlab="Homicide",ylab="Illiteracy",zlab="Fire",theta=20)
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
VX2=seq(0,80,length=251); VX3=seq(5,25,length=251)
VY=outer(VX2,VX3,y)
image(VX2,VX3,VY,xlab="Homicide",ylab="Illiteracy")
contour(VX2,VX3,VY,add=TRUE)
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
model_c = lm(Fire~.,data=chicago)
summary(model_c)$r.squared
summary(model_c)$adj.r.squared
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
logLik(model_c)
AIC(model_c, k=2)               # AIC
AIC(model_c, k=log(nrow(cars))) # BIC
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
library(car)
data(prestige)
model_lm = lm(prestige~income+education, data=Prestige)
y=function(x1,x2) predict(model_lm,newdata=data.frame(income=x1,education=x2))
VX1=seq(min(Prestige$income),max(Prestige$income),length=26)
VX2=seq(min(Prestige$education),max(Prestige$education),length=26)
VY=outer(VX1,VX2,y)
persp(VX1,VX2,VY,xlab="Income",ylab="Education",zlab="Prestige",theta=20, shade=.5)
contour(VX1,VX2,VY,xlab="Income",ylab="Education",zlab="Prestige")
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
data(prestige)
model_loess = loess(prestige~income+education, span=.5, degree=1, data=Prestige)
y=function(x1,x2) predict(model_loess,newdata=data.frame(income=x1,education=x2))
VY=outer(VX1,VX2,y)
persp(VX1,VX2,VY,xlab="Income",ylab="Education",zlab="Prestige",theta=20, shade=.5)
contour(VX1,VX2,VY,xlab="Income",ylab="Education",zlab="Prestige")
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
library(mgcv)
model_gam = gam(prestige ~ s(income) + s(education), data=Prestige)
y=function(x1,x2) predict(model_gam,newdata=data.frame(income=x1,education=x2))
VY=outer(VX1,VX2,y)
persp(VX1,VX2,VY,xlab="Income",ylab="Education",zlab="Prestige",theta=20, shade=.5)
contour(VX1,VX2,VY,xlab="Income",ylab="Education",zlab="Prestige")
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(model_gam)
```

## Penalisation

$$
\widehat{\boldsymbol{\beta}} = \text{argmin}\left\lbrace
\sum_{i=1}^n \left[y_i- \left(\beta_0+\sum_{j=1}^k \beta_j x_{j,i} \right)\right] + \color{red}{\lambda} \sum_{j=1}^k  \beta_j ^2\right\rbrace
$$
with an explicit solution $\widehat{\boldsymbol{\beta}}=(\boldsymbol{X}^{\text{T}}\boldsymbol{X}-\color{red}{\lambda} \mathbb{I})^{-1} \boldsymbol{X}^{\text{T}}\boldsymbol{Y}$.

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
library(MASS,verbose=FALSE,quietly=TRUE,warn.conflicts=FALSE)
model_ridge <- lm.ridge(Fire ~ ., data=chicago, lambda=1)
```

see more generally <a href=https://en.wikipedia.org/wiki/Tikhonov_regularization>Tikhonov regularization</a></span>


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
mse <- NULL
n=100
v <- matrix(c(0,coefficients(model_c)[-1]), nr=n, nc=4, byrow=TRUE)
kl <- c(1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2, 2e-2, 5e-2, 
        .1, .2, .3, .4, .5, .6, .7, .8, .9, 1, 1.2, 1.4, 1.6, 1.8, 2)
for (k in kl) {
  r <- matrix(NA, nr=n, nc=4)
  for (i in 1:n) {
    boot_c <- chicago[sample(1:nrow(chicago),nrow(chicago),replace=TRUE),]
    r[i,2:4] <- model_ridge <- lm.ridge(Fire ~ ., data=boot_c, lambda=k)$coef
    r[i,1] <- mean(boot_c[,"Fire"])
  }
  mse <- append(mse, apply( (r-v)^2, 2, mean )[2])
}
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot( mse ~ kl, type='l' )  
```

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
step(lm(Fire ~ .,data=chicago))
```

<a href=https://en.wikipedia.org/wiki/Lasso_(statistics)>LASSO</a></span> (least absolute shrinkage and selection operator)
$$
\widehat{\boldsymbol{\beta}} = \text{argmin}\left\lbrace
\sum_{i=1}^n \left[y_i- \left(\beta_0+\sum_{j=1}^k \beta_j x_{j,i} \right)\right] + \color{blue}{\lambda} \sum_{j=1}^k \vert\beta_j\vert\right\rbrace
$$

```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
library(glmnet,verbose=FALSE,quietly=TRUE,warn.conflicts=FALSE)
fit = glmnet(x = as.matrix(chicago[,2:4]), y = chicago[,1], family = "gaussian", alpha = 1)
```




```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
plot(fit, xvar="lambda", label = TRUE )
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
step(model)
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
model_acp=lm(Fire ~ princomp(cbind(X_1,X_2,X_3))$scores[,1:3],data=chicago)
predict(model_c)[1:5]
predict(model_acp)[1:5]
```


```{r, fig.width = 10, fig.height = 5, fig.align = "center", tidy.opts=list(width.cutoff=70), message = FALSE}
model_acp=lm(Fire ~ princomp(cbind(X_1,X_2,X_3))$scores[,1:2],data=chicago)
predict(model_c)[1:5]
predict(model_acp)[1:5]
```

## Donnees pour s'entrainer

Munich Rent Index, from Fahrmeir *et al.* (2013) Regression: Models, Methods and Applications
```{r}
base = read.table("http://freakonometrics.free.fr/rent98_00.txt")
str(base)
```
La variable d'interet est `rent_euro`.
